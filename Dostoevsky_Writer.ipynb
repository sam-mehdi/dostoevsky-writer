{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Dostoevsky Writer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EssB23HQvpJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbeUnEulvpKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "461351a3-ea22-43f9-bb58-0398d9c31a76"
      },
      "source": [
        "with open('crime_and_punishment.txt', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(0, len(lines)):\n",
        "        lines[i] = lines[i].lower()\n",
        "    \n",
        "    # Let's separate the data based on sentences\n",
        "    raw_sentences = list()\n",
        "    for line in lines:\n",
        "        for sentence in line.split('.'):\n",
        "            raw_sentences.append(sentence)\n",
        "    print(' --- Sentences before additional cleaning --- ')\n",
        "    print(raw_sentences[:10])\n",
        "    print(f'Number of sentences: {len(raw_sentences)}')\n",
        "    \n",
        "    # Those \\ns sure are annoying...\n",
        "    sentences = list()\n",
        "    for i in range(0, len(raw_sentences)):\n",
        "        raw_sentence = raw_sentences[i]\n",
        "        clean_sentence = raw_sentence.split('\\n')[0]\n",
        "        if clean_sentence:\n",
        "            sentences.append(clean_sentence)\n",
        "            \n",
        "    print(' --- Sentences after additional cleaning --- ')\n",
        "    print(sentences[:10])\n",
        "    print(f'Number of sentences: {len(sentences)}')        "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " --- Sentences before additional cleaning --- \n",
            "['part i\\n', 'chapter i\\n', '\\n', 'on an exceptionally hot evening early in july a young man came out of the garret in which he lodged in s', ' place and walked slowly, as though in hesitation, towards k', ' bridge', '\\n', '\\n', 'he had successfully avoided meeting his landlady on the staircase', ' his garret was under the roof of a high, five-storied house and was more like a cupboard than a room']\n",
            "Number of sentences: 23649\n",
            " --- Sentences after additional cleaning --- \n",
            "['part i', 'chapter i', 'on an exceptionally hot evening early in july a young man came out of the garret in which he lodged in s', ' place and walked slowly, as though in hesitation, towards k', ' bridge', 'he had successfully avoided meeting his landlady on the staircase', ' his garret was under the roof of a high, five-storied house and was more like a cupboard than a room', ' the landlady who provided him with garret, dinners, and attendance, lived on the floor below, and every time he went out he was obliged to pass her kitchen, the door of which invariably stood open', ' and each time he passed, the young man had a sick, frightened feeling, which made him scowl and feel ashamed', ' he was hopelessly in debt to his landlady, and was afraid of meeting her']\n",
            "Number of sentences: 13619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmkeXXZhvpK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "103456a6-5c10-47a3-fb3e-98ebdd400228"
      },
      "source": [
        "# It's already time to tokenize! Let's check the total vocabulary size\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwKOZMQqvpLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We also need to make a 'labels' class\n",
        "# create input sequences using list of tokens\n",
        "max_sentence_length = 100\n",
        "input_sequences = []\n",
        "for sentence in sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sentence_length+1, \\\n",
        "                                         padding='pre', truncating='post'))\n",
        "word_index = tokenizer.word_index\n",
        "num_tokens = len(word_index) + 1\n",
        "\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = keras.utils.to_categorical(label, num_classes=num_tokens)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e22_DnIyazO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "af58a872-c3c9-429d-bf26-07a7f88aa2f7"
      },
      "source": [
        "print(len(label[0]))\n",
        "print(len(predictors[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10733\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZk0DqPIvpLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's use the GloVE word embeddings for this project, available here:\n",
        "# nlp.stanford.edu/data/glove.6B.zip\n",
        "# https://nlp.stanford.edu/pubs/glove.pdf\n",
        "# Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.  GloVe: Global Vectors for Word Representation. \n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding = 'utf-8') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfqb5_7EvpLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5cf6dbce-4518-4cbc-aa71-57a89a143b04"
      },
      "source": [
        "# This code is adapted from here:\n",
        "# https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "# That should do!"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 8920 words (1812 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VPt7rdkvpLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's make the model, first editing the Embedding layer\n",
        "from tensorflow.keras.layers import Embedding\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    input_length=max_sentence_length,\n",
        "    trainable=True,\n",
        ")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huLAQw0_vpL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "752b9afb-0e44-4800-ffaa-6bdab0307a2e"
      },
      "source": [
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.regularizers as regularizers\n",
        "inputs = keras.Input(shape=(max_sentence_length,))\n",
        "embedded_sequences = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(1024, return_sequences = True))(embedded_sequences)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.LSTM(512, return_sequences=True)(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.LSTM(512)(x)\n",
        "#x = layers.Dense(num_tokens/2, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "preds = layers.Dense(num_tokens, activation = 'softmax')(x)\n",
        "model = keras.Model(inputs, preds)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         [(None, 100)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 100, 100)          1073300   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 100, 2048)         9216000   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 100, 2048)         0         \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 100, 512)          5244928   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100, 512)          0         \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10733)             5506029   \n",
            "=================================================================\n",
            "Total params: 23,139,457\n",
            "Trainable params: 23,139,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1gp9f3ZvpL7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "00fda11f-e16c-4e9d-a717-7cfb9dc3a259"
      },
      "source": [
        "history = model.fit(predictors, label, batch_size=64, epochs=10, verbose=2)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3033/3033 - 666s - loss: 6.2833 - accuracy: 0.0691\n",
            "Epoch 2/10\n",
            "3033/3033 - 667s - loss: 5.3824 - accuracy: 0.1400\n",
            "Epoch 3/10\n",
            "3033/3033 - 667s - loss: 4.9588 - accuracy: 0.1668\n",
            "Epoch 4/10\n",
            "3033/3033 - 668s - loss: 4.6732 - accuracy: 0.1824\n",
            "Epoch 5/10\n",
            "3033/3033 - 668s - loss: 4.4416 - accuracy: 0.1955\n",
            "Epoch 6/10\n",
            "3033/3033 - 667s - loss: 4.2223 - accuracy: 0.2084\n",
            "Epoch 7/10\n",
            "3033/3033 - 669s - loss: 4.0106 - accuracy: 0.2225\n",
            "Epoch 8/10\n",
            "3033/3033 - 670s - loss: 3.8169 - accuracy: 0.2377\n",
            "Epoch 9/10\n",
            "3033/3033 - 670s - loss: 3.6262 - accuracy: 0.2557\n",
            "Epoch 10/10\n",
            "3033/3033 - 670s - loss: 3.4496 - accuracy: 0.2757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfijMTuMvpMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "30306043-0d28-4beb-8d55-0ce510928849"
      },
      "source": [
        "# Using the model to generate some new text\n",
        "seed_text = \"upon\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sentence_length, padding='pre')\n",
        "  predicted = np.argmax(model.predict(token_list, verbose=0), axis=1)\n",
        "  output_word = ''\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += ' ' + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upon my word i am not going to be a card chantant and have a great deal of it and i am not going to be a card chantant and have a debate to her to day and i am not going to be a card chantant and have a debate to him to day ” he said suddenly raising his head and laughing maliciously gibing at the axe “i am not going to be crucified crucified on the kingdom of heaven for me and i am glad to be crucified crucified on the children and sister living with a subscription\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U_qtzgaEKDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# quite strange text, but it sure does sound like Dostoevsky!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}